# Data-Driven-Exam
due 17th of May

* 두 변수의 연관성 * 
 두 변수 간에 관련이 있다.
 두 변수가 서로 독립이다.(관계없음)
 
 연관성의 측도 -> 공분산 / 상관계수 
 
 예시 1) 
 
<img width="949" alt="Screenshot 2022-05-11 at 11 25 46" src="https://user-images.githubusercontent.com/84698855/167828700-717bef60-b25c-4402-ad51-7364dc85ffd8.png">

두 변수 (키 / 몸무게) 상관관계를 알아보자. 키가 크면 몸무게가 크다. 두 변수는 상관관계가 있다. 그림에서도 보이듯이 x축 즉 키의 값이 커질수록 y축 몸무게도 커진다.
이 뜻은 두 변수는 양의 상관관계이다. (x 축이 작아지면 y 축도 작아진다) 

 예시 2)
 
반대로 한쪽의 변수가 커지고 한쪽의 변수가 작아지면 음의 상관관계 기울기를 보여준다 

<img width="931" alt="Screenshot 2022-05-11 at 11 29 22" src="https://user-images.githubusercontent.com/84698855/167829328-ab833d9c-206c-4039-bef6-707909eda16f.png">

예시 3)

그러면 두 변수가 아무 상관관계가 없는 그래프는? (두 변수가 전혀 영향을 주지 않는 즉 두 변수가 서로 독립관계일때)

<img width="913" alt="Screenshot 2022-05-11 at 11 31 10" src="https://user-images.githubusercontent.com/84698855/167829623-34b1d2df-b3ab-46ba-8b78-37175fa8a8d7.png">


*공분산 (variance)*

[양의 상관]

<img width="976" alt="Screenshot 2022-05-11 at 11 43 49" src="https://user-images.githubusercontent.com/84698855/167831725-fb1aef48-9e33-44a1-a8da-dd33486283e3.png">

-> 만약 평균을 x= 170 y=70 으로 놓았을때, 임의의 점 하나(양의 상관쪽)를 선택 후 그 점의 위치하는 평균값과 set평균값을 빼준다. 
양의 상관관계이면 공분산의 값이 커진다. 

[음의 상관]

<img width="1001" alt="Screenshot 2022-05-11 at 11 47 22" src="https://user-images.githubusercontent.com/84698855/167832296-6c557420-18fa-42c5-8898-618e2cda29ca.png">

양의 상관과 반대인 상황 즉 음의 상관관계이면 공분산 값이 작아진다. 

하지만 공분산이 크다고 무조건 연광성이 높은가? 공분산은 단위,범위에 영향을 받아서 표준화를 시켜줄 필요가 있다. 

<img width="776" alt="Screenshot 2022-05-11 at 11 50 42" src="https://user-images.githubusercontent.com/84698855/167832793-c1809ba4-a750-4008-ba12-fb85abacbe66.png">




Regression (회기분석)

* 회기란 말은 어딘가로 돌아간다는 의미
* 도대체 어디로 돌아가길래 이런 이름이 붙었을까 ?

회기분석의 목적 

* 주어진 (독립)변수로 (종속)변수를 예측학 위해 -> 독립변수란(Independent) : 영향을 주는 변수(원인) / 종속변수란(Dependent) : 영향을 받는 변수(결과) 

* 단순회귀(Simple regression) : 독립변수 1개 & 종속변수 1개
* 다중회귀(Multiple regression) : 독립변수 2개 이상 & 종속변수 1개


<img width="1245" alt="스크린샷 2022-05-03 15 22 46" src="https://user-images.githubusercontent.com/84698855/166473368-bf5f28c3-b3be-416a-ad75-0ee8cfd10230.png">

위에 데이터를 토대로 새로운 데이터를 추론할 수 있다. (different value of independent)

-> 새로운 데이터를 예측하기 위해 필요한 것 : 추세선 (이 추세선을 예측하느 것이 회귀분석의 최종 목적 !)


추세선 
->  y = a + bx ( y is dependent x is independent ) we need a and b value. [ a 는 절편 b 는 기울기 ] 
a 와 b 의 값을 구하는것이 회귀분석 

<img width="792" alt="Screenshot 2022-05-03 at 15 35 48" src="https://user-images.githubusercontent.com/84698855/166474328-bed5aca1-7cc4-4b78-8863-33b19421257e.png">




y hat = a + bx is 추세선 / y = a + bx is 점들( i 들은 점들의 개별의 값)


<img width="797" alt="Screenshot 2022-05-03 at 15 39 27" src="https://user-images.githubusercontent.com/84698855/166475005-2e1c96cf-378d-4f6b-a4f5-aeb9476053fe.png">





가장 완벽한 추세선 즉 좋은 추세선은 에러의 차이가 적은것 
-> 에러를 줄이기 위해서 최소제곱법을 이용한다 ( 즉, 점들은 + 또는 - 에 위치해서 제곱을 시켜줌으로써 뭉개지지 않게 하는 것이 목표, 오차를 적게만드는 것 ) 모든 점들을 제곱후 점의 개수만큼 나누기
[ 모든 것을 다 더하면 가능 ] 
cost(a,b) 의 값이 추세선과 가까울 수록 0의 수렴 반대로 0에서 커질수록 오차 범위 큼

오차값 = 측정값 - 예측값 ( e = yi - yi hat) 


******************************************************************************************************************************************************


Classification (분류) 

말 그대로 분류를 뜻하는 classfication 은 지도학습의 일종으로 기존에 존재하는 데이터의 category관계를 파악하고, 새롭게 관측된 데이터의 category를 스스로 판별하는 과정이다. 

예를들어 문자를 판별하여, 스팸 보관함으로 분류하는것과 같은 단일분류와, 수능 점수가 몇 등급에 해당하는지 판별하는 종류의 다중분류가 있다. 다중분류는 비지도학습의 clustering 과 비슷하지만, 
가장 큰 차이점은 category의 도메인이 정의되있는가 그렇지 않은가 이다. 

지도학습의 classification 은 이미 정해진 카테고리(레이블) 안에서 학습하여 새로운 데이터르 분류하지만, 비지도학습의 clustering은 정해지지 않은 카데고리(레이블)르 원하는 만크 생성하여, 분류하는 것이 가장 큰 차이점이다. 


Classification 알고리즘 종류 

(1) K -nearest neighbor 
KNN 은 데이터를 분류하고 새로운 데이터 포인트의 카테고리를 결정할 때 K 개의 가장 가까운 포인트를 선점하고 그중 가장 많이 선택된 포인트를 카테고리로 이 새로운 데이터를 분류하는 방법이다. 
KNN 에서 고려해야 할 사항은 알고리즘의 핵심 부분의 대상 포인트와의 거리 대한 측정이고, 이를 계산하는 방법으로 무조건 유클리드 거리 측정 방식을 사용하는 것을 자제해야한다. 

모든 데이터 열을 이처럼 같은 방식으로 처리하면 생각하지 못한 변수에 의해 오류가 생길 수 있으므로 거리의 제곱을 합산학 전 각 카테고리에 대한 평규 거리를 빼고 계산하는 방식과 같은 다양한 거리 계산 알고리즘의 대한 논의가 필요하다. ex) 실수 데이터의 경우 유클리드 거리 측정방식을 사용하고 범주형 혹은 이진 데이터와 같은 유형의 데이터는 해밍거리 측정 방식을 사용한다. 


<img width="674" alt="스크린샷 2022-05-05 16 15 06" src="https://user-images.githubusercontent.com/84698855/166955673-8be6c5dc-aba7-4a84-8bfc-67490dab95e2.png">


* 두점 사이의 거리공식 이해하기 * [ 유클리드 거리 / 해밍턴 거리 / 맨하탄 거리 ]

두점 사이의 거리를 구하는 이유 : 거리는 일종의 유사도(similarity) 개념이기 때문이다. 거리가 가까울수록 그 특성들이 비슷하다는 뜻이다. 

1. 유클리드 거리 

<img width="820" alt="Screenshot 2022-05-08 at 15 51 04" src="https://user-images.githubusercontent.com/84698855/167301900-55daa2a8-a2b6-43a3-a3fa-afad2ca25d02.png">

2. 맨하탄 거리 

맨하탄 거리도 유클리드와 유사하지만, 각 차원의 차를 제곱해서 사용하는게 아니라 그냥 절대값을 바로 합산하는 것. 

<img width="594" alt="Screenshot 2022-05-08 at 15 53 58" src="https://user-images.githubusercontent.com/84698855/167302009-5374c687-de35-42d2-8511-a9ec8750d7b6.png">


3. 해밍턴 거리

해밍턴 거리는 유클리드와 맨하탄 거리와 조금 다르다. 해밍턴 거리는 각 차원마다 차이를 찾는게 아니라 '정확히 같은지' 여부만 고려한다. (주로 맞춤법 검사와 같은 알고리즘으로 쓰임)
ex) 단어 'there' 과 "thete" 사이에 해밍 거리는 1이다. 각 문자가 차원이며 이 예시에서 각 차원은 네번째 문자 r과 t를 제외하고 동일한 값을 갖고 있기 때문이다. 


(2) Naive Bayes 

나이브 베이즈는 확률을 사용한다. 
ex) 만약, 심슨에 대한 feature에 age와 sex 2개가 존재하는 상황에서, 심슨으 나이, 성별에 따라 살아남을 확률은 알마인지 계산 


<img width="656" alt="스크린샷 2022-05-05 16 17 42" src="https://user-images.githubusercontent.com/84698855/166956142-f4dc5af7-e89b-4f45-a3b6-4f896d750112.png">

******************************************************************************************************************************************************

Clustering & K - mean clustering 

-> 만약 우리가 다루는 데이터에 레이블이 붙어 있다면 지도학습, 즉 미리 가지고 있는 데이터와 레이블을 기반으로 예측이나 분류를 수행하는 모델을 만들수 있다. 
그러나 실제로는 레이블이 없는 경우가 더 많다. 물론 이렇게 별도의 레이블이 없는 데이터 안에서 패턴과 구조를 발견하는 비지도 학습도 머신러닝의 큰 축이고, 그 중 가장 대표적인 비지도 학습 기술이
바로 Clustering 이다. 

참고로 지도학습 Classfication 과는 엄연히 다른 것이다. 분류작업은 미리 레이블이 붙어 있는 데이터들을 학습해서 그걸 바탕으로 새로운 데이터에 대해 분류를 수행하지만, clustering은 레이블을
모르더라도 그냥 비슷한 속성을 가진 데이터끼리 묶어주는 역할을 하기 때문이다. 

군집화의 목표 

군집화의 목표는 서로 유사한 데이터들은 같은 그룹으로, 서로 유사하지 않은 데이터는 다른 그룹으로 분리하는 것이 된다. 그러면 자연스럽게 2개의 질문이 따라온다.

1) 몇개의 그룹으로 묶을 것인가?
2) 데이터의 "유사도" 를 어떻게 정의할 것인가 ( 유사한 데이터란 무엇인가?)

K- means 군집화의 원리 

- K 는 데이터 세트에서 찾을 것으로 예상되는 클러스터(그룹) 수를 말한다. 
- Means 는 각 데이터로부터 그 데이터가 속한 클러스터의 중심까지의 평균거리 ( 이 값을 최소화 하는것이 알고리즘의 목표 !)

K- means 에서는 이걸 구현하기 위해 반복적인(iterative) 접근을 취한다.

1. 일단 K 개의 임의의 중심점(centroid)을 배치하고 
2. 각 데이터들을 가장 가까운 중심점으로 할당한다 ( 일종의 군집을 형성)
3. 군집으로 지정된 데이터들을 기반으로 해당 군집의 중심점을 업데이트한다. 
4. 2,3번 단계를 그래서 수렴이 될때까지 , 즉 더 이상 중심점이 업데이트 되지 않을 때 까지 반복한다. 

<img width="786" alt="Screenshot 2022-05-08 at 15 39 28" src="https://user-images.githubusercontent.com/84698855/167301445-0c88c50d-c46e-4ec2-9b64-3d1c865a5736.png">


여기서 일단 K 값은 2이다. 그래서 (b)에서 일단 중심점 2개를 아무데나 찍고, (c) 에서는 각 데이터들을 두개 점 중 가까운 곳으로 할당한다. (d)에서는 그렇게 군집이 지정된 상태로 중심점을 업데이트 한다. 그리고 (e) 에서는 업데이트 된 중심점과 데이터들의 거리를 구해서 군집을 다시 할당한다. -> 이 작업을 계속 반복 !!

결론: 이렇게 군집화를 해놓으면 새로운 데이터가 들어와도 그게 어떤 군집에 속할지 할당해줄 수 있게 되는 셈이다. 



******************************************************************************************************************************************************

Gaussian Mixture Model (GMM) with clurstering 

[우리가 신경써야 할 것 : 평균 / 분산 / *잠재변수(z)]

[여기서 mixture model 이라는 것의 뜻은 기본분포를 선형결합해서 만든 분포라는 뜻이다.] -> 가우시안 분포 선형결합
* 이런 식으로 모델을 딱 가정해버리면 모델에 들어가는 파라미터를 찾는 식으로 밀도를 추정하게 된다. *

<img width="702" alt="Screenshot 2022-05-07 at 10 23 42" src="https://user-images.githubusercontent.com/84698855/167248078-f496e96b-debf-4f2e-bea8-d27cf2c34dc8.png">

가우시안 분포가 여러 개 혼합된 clustering 알고리즘이다. 현실에 존재하는 복잡한 형태의 확률 분포를 그림과 같이 K개의 gaussian distribution을 혼합하여 표현하자는 것이 GMM의 기본 아이디어이다. (이때 K는 데이터를 분석하고자 하는 사람이 직접 설정해야한다.) 

주어진 데이터 x에 대해 GMM은 x가 발생할 확률을 아래 식 과 같이 여러  Gaussian probability density function 의 합으로 표현한다. 
<!-- 
<img width="663" alt="Screenshot 2022-05-07 at 10 27 42" src="https://user-images.githubusercontent.com/84698855/167248212-c273fb17-9f06-46a5-9f05-b1c8f90163a1.png"> -->

<img width="478" alt="Screenshot 2022-05-07 at 10 40 30" src="https://user-images.githubusercontent.com/84698855/167248747-b72572c9-44e5-4559-9633-9b89f892f209.png">



위에 식에서 mixing coefficient(Covariance matrix) 라고 하는 Pie k 는 K번째 gaussian distribution이 선택될 확률을 나타낸다. 따라서 pie k는 아래의 두 조건을 만족해야 한다. 이는 각각의 gaussian density가 얼마만큼의 비율로 들어가는지를 뜻하고 모두 합쳐 1이 되어야 한다. (가우시안 함수의 크기를 결정한다고 볼 수 있음) 

<img width="450" alt="Screenshot 2022-05-07 at 10 30 33" src="https://user-images.githubusercontent.com/84698855/167248328-a79de133-d4ce-44aa-8393-2ce1d03ad025.png">




GMM을 학습시킨다는 것은 주어진 데이터 X = {x1,x2, .... xn} 에 대하여 적잘한 pie k, mu k, sigma k 를 추정하는 것과 같다. 

******************************************************************************************************************************************************

Responsibility (책임값)

앞서 가우시안 성분들을 선형으로 중첩시킨 가우시안 혼합모델을 사용하면 단일 가우시안을 이용할 때 보다 더 다양한 종류의 밀도모델을 표현할 수 있따는 것을 살펴봄. 이 파트부터 GMM의 파라미터를 찾는 것을 보일텐데, 그때 EM알고리즘의 Estep에서 사용되는 responsibility 라는 확률값을 살펴본다. 


<img width="578" alt="Screenshot 2022-05-07 at 10 55 59" src="https://user-images.githubusercontent.com/84698855/167249261-89ed5424-c15f-433e-b30a-04e227e9f8f4.png">


먼저 데이터포인트 Xn이 K개의 가우시안중에 몇번째 가우시안에서 왔는지에 대한 확률을 알고싶다고 하자 그렇다면 아래와 같이 나타낼 수 있다. 

<img width="319" alt="Screenshot 2022-05-07 at 10 57 13" src="https://user-images.githubusercontent.com/84698855/167249295-0170bf11-d6f2-4c88-8788-8f3ef48320e2.png">


이 경우에는 z는 잠재변수(latent variable)인데, x가 실제 k번째 가우시안에서 왔으면 1 그렇지 않으면 0 두가지 값만 가질 수 있다. (아래와 같이 표현을 할 수 있다) 

<img width="275" alt="Screenshot 2022-05-07 at 10 59 28" src="https://user-images.githubusercontent.com/84698855/167249400-5ffcf32d-659d-4d84-8ef9-e5d46f27d62a.png">


이 말은 mixing coefficient pie k 가 k번째 가우시안에서 오는 점을 관측할 전체확률과 같다는 것을 의마한다. 
pie k는 가우시안 함수의 크기를 나타내기 때문에 이 값이 클수록 확률이 더 높을 것이라는 의미가 있다. k개의 잠재변수 z를 모아서 벡터로 표현할 수 있다. 

x가 나올 확률같은 경우에 처음에 정의했던 gaussian mixture로 나타댈 수 있다. 

<img width="584" alt="Screenshot 2022-05-07 at 11 04 20" src="https://user-images.githubusercontent.com/84698855/167249603-62a619f0-eb40-4f7d-b538-7f6d2d2794c8.png">


이제 정의한 것들을 이용하여 구하고자했던 데이터포인트 xn이 k개의 가우시안중에 몇번째 가우시안에서 왔는지에 대한 확률 식을 완성해보자 (베이즈 룰 이용) 

<img width="581" alt="Screenshot 2022-05-07 at 11 06 51" src="https://user-images.githubusercontent.com/84698855/167249667-9a453bd7-14a0-4420-8cd2-46f8ce2d611c.png">


******************************************************************************************************************************************************

푸리에 발견 / 급수 / 변환 / 공간 

* 푸리에의 발견 : 같은 형태를 반복하는 주기를 가진 파동은, 아무리 복잡한 것이라도 단순한 파동이 잔뜩 결합해 이루어진다. 

* 푸리에 급수: 숫자들을 나열하여 합한 것. 

<img width="697" alt="Screenshot 2022-05-09 at 11 14 34" src="https://user-images.githubusercontent.com/84698855/167389760-85e52d0e-6db4-4f34-bd02-ed474c66cf52.png">


위 식은 삼각혐수를 이용하여 (복잡한 파동) = (단순한 파동1) + (단순한 파동2) + .... 

1) a0 의 존재 : 정현파 파동 간에 언제나 0을 중심으로 진동하는 파동이기 때문에 0 이 중심이 아닌 파동은 표현할 수 없다. 복잡한 파동은 중심이 0이 아닐 수 도 있으므로 중심이 0 이 아닌 파동을 표현하기 위해서 a0은 필수 존재 이다. 

2) 주기 적인 파동은 기본주파수가 정수배인 주파수의 파동들로 이루어져 있다. 
-> 단순한 파동 모두 w(omega), 2w, 3w ... 처럼 기본주파수의 정수배로 이루어져 있다. 복잡한 파동 함수 f(t)가 주기를 가진 파동이기 때문에 단순한 파동들도 주기를 반복해야 하고 이를 만족하려면 기본 주파수의 정수배가 되어한다. 

위에 식을 sigma를 이용하면 다음과 같이 정리할 수 있다. 

<img width="653" alt="Screenshot 2022-05-09 at 11 20 24" src="https://user-images.githubusercontent.com/84698855/167390759-fd477af8-e169-433d-8bd2-6258ecb600d2.png">

3) an 과 bn 
-> an 과 bn 이 의미 하는 것은 각각은 단순한 파동들이 얼마나 들어있는지를 나타낸다. 평면 상의 점들도 x축과 y축 성분이 얼마만큼 있냐에 따라 전혀 다른 점이 된다. 
이와 마찬가지로 같은 종류의 단순한 파동으로 이루어졌다 하더라도 각 단순 파동들이 얼만큼 들어있냐에 따라 파동의 종류는 완전히 달라진다. 



1. 푸리에의 변환( 중요 !!!!) 
푸리에의 변환이 무엇이고 어디에 쓸 수 있는지, 그리고 어떻게 쓸 수 있는지 직관적 이해와 유용한 성질들, 영상처리 응용, 등등 필요한 사항들을 알아보자 !

푸리에의 변환을 직관적으로 설명하면 임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현하는 것이다. (주기함수 : 어떤 파형이 같은 값을 다시 반복하는데 걸리는 가장 작은 시간 대표적으로 사인 / 코사인 )


좀더 들어가면 푸리에 변환에서 사용하는 주기함수 사인,코사인은 삼각함수이며 푸리에 변환은 고주파부터 저주파 까지 다양한 주파수의 대역의 사인 코사인 함수들로 원본 신호를 분해하는 것 


<img width="575" alt="Screenshot 2022-05-09 at 11 44 04" src="https://user-images.githubusercontent.com/84698855/167394524-42f647fd-c1c7-4f92-b8b9-78fc0870691a.png">   [그림1]


위에 그림에서 붉은 색 신호는 입력 신호이고 뒤에 파란색 신호들은 푸리에 변환을 통해서 얻어진 주기함수 성분들이다. 각각의 주기함수 성분들은 고유의 주파수와 강도를 가지고 있으며 이들을 모두 합치면 원본 붉은색 신호가 된다. 여기서 입력 신호는 전파, 음성 신호 등과 같이 시간축(time)에 대해 정의된 신호일 수도 있고 이미지 등과 같이 공간축(space) 에 대해 정의된 신호일 수도 있다. 특히 통신 분야에서는 푸리에 변환을 f(t) : f(f) [time domain -> frequency domain] 으로의 변환이라고 하고, 컴퓨터 비젼, 영상 처리 쪽에서는 f(s) : f(f) [spatial domain -> frequency domain] 으로 변환. 

** 푸리에 변환의 대댄한 점은 입력 신호가 어떤 신호이든지 관계없이 임의의 입력 신호를 사인 코사인 주기 함수들의 합으로 항상 분해 할 수 있다는 것이다 ! -> 푸리에 변환식 **


2. 푸리에 변환 수식적 이해 


<img width="518" alt="Screenshot 2022-05-09 at 11 50 47" src="https://user-images.githubusercontent.com/84698855/167395570-3a0fdc4c-e47d-4e79-a0a5-cbe7f46fbc15.png">


여기서 j는 허수단위 (j= root -1) , f(x) 는 원본 입력 신호, e^j2pieux 는 주파수 u인 주기함수 성분, f(u)는 해당 주기함수 성분의 계수 [보통 clockwise - anticlockwise +] 

일단 식을 있는 그대로 해석하면 식(1)은 입력신호 f(x)가 주기함수들(e^j2pieux)의 합으로 표현(분해)된다는 의미. 그리고 식(2)는 f(x)를 주기함수 성분으로 분해했을 때의 계수 f(U)가 식(2)로 주어진다는 의미. 위 그림(1)과 연관해보면 e^j2pieu는 f(x)를 구성하는 주기함수들의 성분이고 (블루들의 합 = 레드) f(u)는 해당 주기함수 성분의 강도(amplitute) [coefficient = 강도] 


푸리에 변환에 대한 일반적인 설명 방식은 두번째 식(2)을 푸리에 변환 이라고 정의하고 첫번째 식(1) 을 푸리에의 역변환이라고 정의한다. 


궁금증 ) 그러면 과연 e^j2pieu(파랑이들의 모임) 이부분은 어떻게 추론 하고 쉽게 생각 할 수 있을까?

-> 이를 추론하기 위해서는 오일러의 공식이 필요하다. 

<img width="327" alt="Screenshot 2022-05-09 at 12 06 10" src="https://user-images.githubusercontent.com/84698855/167397659-440c755a-76b9-4aee-a797-1b0317082481.png">

오일러 공식은 복소지수함수를 삼각함수로 변환할 수 있도록 하는 유명한 식이다. 오일러 공식을 이용하면 위 식(1) [역변환]의 e^j2pieu는 실수부가 cos(2piuux), 허수부가 sin(2pieux)인 주기함수 보여줌.


<img width="335" alt="Screenshot 2022-05-09 at 12 11 28" src="https://user-images.githubusercontent.com/84698855/167398332-6d108ca5-db11-4bf2-9c28-8a44535667c5.png">   [theta : e^j2pieu]

여기서 cos(2pieux), sin(2pieux) 모두 주기(period)가 1/u, 주파수(frequency) u인 주기함수이므로 결국 e^j2pieu 는 주파수 u인 정현파의 복소지수함수 표현임을 알 수 있다. 
[ 어떤 주기라는 시간마다 좌변과 우변이 같은 값을 갖게 되는 것. 어떤 주파수는 이것이 1초동안 얼마나 반복하냐 (주기의 역순) ] 그래서 위식에서 1/u 가나옴 




3. 이미지 신호에서의 푸리에 변환 (2D transform) 

푸리에 변화을 영상처리에 적용하기 위해서는 이미지(영상신호)가 가지고 있는 몇 가지 차이점을 인지해야 한다. 먼저, 이미지는 2차원의(x 축 방향의 변화와 y축 방향의 변화가 동시에 포함된) 신호이기 때문에 2차원에서의 정의되는 푸리에 변환이 필요하다. 

<img width="475" alt="Screenshot 2022-05-09 at 16 52 31" src="https://user-images.githubusercontent.com/84698855/167448609-93ae8362-8883-42f3-9a38-b8302f22f99d.png">

단 여기서 F(u,v)는 x축 방향으로 주파스 u, y축 방향으로 v인 주기함수 성분의 계수이다. 그리고 그 값은 식(6)에 의해 계산된다. 
그런데 이미지는 연속이 아닌 이산 신호이다. [classification 개념확인] 그리고 한정된 유한구간에서 정의되는 신호이다. 따라서, 이산 데이터에서 정의되는 푸리에 변환이 필요하다. 
W x H(width x height) 크기의 이미지 f(x,y)에 대한 푸리에 변환은 다음과 같이 정의된다. 


<img width="543" alt="Screenshot 2022-05-09 at 17 08 06" src="https://user-images.githubusercontent.com/84698855/167451502-c25d89a4-1c61-45c4-8e70-63f264408362.png">


식(7)에서  ej2π(ux/W+vy/H)는 x축방향으로 주파수가 u/W, y축방향으로 주파수가 v/H 인 주기함수이다(by 오일러 공식). 일반적인 푸리에 변환식과는 달리 W와 H로의 나누기가 들어있음에 유의해야 하며 이는 데이터가 정의된 구간을 하나의 단위 주기(unit period)로 만드는 효과가 있다. 

여기서 2D이미지를 어떻게 신호로 해석할 수 있는지, 그리고 2D 정현파 ej2π(ux/W+vy/H)가 도대체 어떤 모습일지 궁금증 생김 
-> 1. 이미지를 신호로 해석하는 문제는 x또는 y축을 시간축으로 놓고 좌표의 변화에 따라 변화는 이미지 필셀의 밝기 변화를 신호로 생각하면 쉽게 이해할 수 있다. 다음으로, 2D에서 정의되는 정현파의 모습은 
아래 그림과 같이 모든 방향으로의 단면이 정현(sinusoidal)이 되는 물결 형태의 파동을 생각하면 된다.



<img width="620" alt="Screenshot 2022-05-09 at 17 32 23" src="https://user-images.githubusercontent.com/84698855/167455792-0735d6e3-abd2-4f11-997d-8ed011f1d8cf.png">

앞서 그림1의 1D 푸리에변환의 경우와 유사하게 생각해 보면, 이미지에 대한 푸리에 변환은 그림2와 같은 형태의 다양한 2D 정현파들의 합으로 이미지를 분해하여 표현하는 것으로 이해할 수 있다. 

이미지에 대한 푸리에 변환에서 한가지 주의해야 할 것은 푸리에 변환의 계수 F(u,v)가 ej2π(ux+vy)의 계수가 아니라 ej2π(ux/W+vy/H)의 계수라는 점이다. 즉, 이산 푸리에 변환에서 F(u,v)는 주파수 u,v 성분이 아니라 주파수 u/W, v/H 성분에 대한 계수를 나타낸다. 



4. 푸리에 스펙트럼과 페이즈 

이제 실제로 푸리에 변환을 통해 얻어지는 F(u,v)값들이 어떤 의미를 가지며 어떤 형태를 갖는지 살펴보자. 푸리에 변환을 통해 얻어지는 F(u,v)는 복소수 이미 실수부와 허수부로 구성된다. 
(1차원 푸리에 변환의 경우도 마찬가지이다.)

<img width="292" alt="Screenshot 2022-05-09 at 17 55 08" src="https://user-images.githubusercontent.com/84698855/167459535-52e5939e-7b4f-426b-8087-8bf9499811c0.png">

이때, 복소수 F(u,v)의 크기 |F(u,v)| 를 푸리에 변환의 스펙트럼 또는 magnitude 라고 부르고, F(u,v)의 각도를 phase angle 또는 pahse spectrum이라고 부른다. 

<img width="405" alt="Screenshot 2022-05-09 at 18 03 32" src="https://user-images.githubusercontent.com/84698855/167460799-d66dd735-784a-4666-9b4d-d4efcdd074cc.png"> [ how to get Magnitude of the FT and Phase angle]


A. 푸리에 스펙트럼
  먼저 푸리에 스펙트럼이란 무엇인지 살펴보자. 푸리에 스펙트럼은 해당 주파스 성분이 원 신호(이미지)에 얼마나 강하게 포함되어 있는지를 나타낸다. 
  W x H 이미지를 푸리에 변환(Fourier transform)하면 식 (7), (8)에 의해 W x H의 F(u, v), u = 0, ..., W-1, v = 0, ..., H-1 가 얻어진다. 따라서, |F(u, v)|를 픽셀값으로 잡으면 아래 예와 같이 푸리에 스펙트럼을 원본 이미지와 동일한 크기의 이미지로 시각화할 수 있다.
  
  

<img width="581" alt="Screenshot 2022-05-09 at 20 29 05" src="https://user-images.githubusercontent.com/84698855/167483407-d20a9053-3cb7-40df-b713-3285fb98aec1.png"> [ a : 입력이미지 b: 푸리에 스펙트럼 c: shifted 스펙트럼 ]

푸리에 스펙트럼을 이미지로 시각화하는 데에는 2가지 문제점이 있다. 먼저 푸리에 스펙트럼은 저주파 영역에서 큰 값을 갖는 반면에 대부분의 다른 영역은 0에 가까운 값을 갖는다. 따라서 푸리에 스펙트럼을 그대로 이미지로 시각화하면 검은 바탕 위에 흰점 하나만 존재하는 형태가 된다. 이러한 문제를 해결하기 위해서 스펙트럼을 이미지로 표현할 때에는 위 그림(b)처럼 스펙트럼에 log를 취하는 것이 일반적이다. 
다음으로, 원래의 스펙트럼 이미지는 그림 b처럼 모서리로 갈수록 값이 높아지기 때문에 스펙트럼의 형태를 파악하기 힘들다. 따라서, 이러한 문제를 해결하기 위해 그림 c처럼 원점이 중심에 오도록 스펙트럼의 위치를 이동시킨 형태의 이미지를 사용하는것이 일반적이다. 앞으로 푸리에 스펙트럼 이미지라 하면 c와 같은 shifted spectrum 이미지를 생각하면 된다. 

B. 푸리에 스펙트럼의 해석
푸리에 스펙트럼은 해당되는 주파서 성분의 강도를 나타낸다고 설명했는데, 정말 그런지 그리고 이 값이 이미지 도메인에서 어떻게 해석될 수 있는지 실제 예를 통해 알아보자 !
아래 예는 이미지에 인위적으로 주기성분을 추가하였을 때 주파수 공간에서의 푸리에 스펙트럼이 어떻게 변하는지를 보여준다. 원본 이미지의 해상도는 205X205 픽셀이며 따라서 스펙트럼 이미지도 동일하다. 


<img width="579" alt="Screenshot 2022-05-09 at 20 41 55" src="https://user-images.githubusercontent.com/84698855/167485312-c2a7d302-b4ff-479a-b585-6595359ce606.png">

먼저, 그림 a는 원본 이미지 및 대응되는 푸리에 스펙트럼 이미지를 보여준다. 그림의 예와 같이 일반적인 푸리에 스펙트럼 이미지는 원점 F(0,0) 주변의 저주파 영역에서 강한 피크(peak)가 나타나고 원점에서 멀어질수록 즉, 고주파 영역으로 갈수록 값이 급격히 작아지는 형태를 갖는다. 

그림 b는 a의 이미지에 5픽셀 간격의 수평선을 인위적으로 추가한 경우이다. 그러면 주파수 공간에서는 그림과 같이 F(0,41), F(0,82)에 강한 피크가 나타난다. 앞서 이산 푸리에 변환에서 F(u,v)는 x축 주기  W/u 픽셀, y축 주기 H/v 픽셀인 주기성분의 계수라 했다. 그러면, F(0, 41)은 주기가 x축 방향 205/0 = ∞, y축 방향 205/41 = 5 픽셀인 주기성분에 대응된다. 그리고 이것은 그림 5(b)를 만들 때 사용한 수평선의 주기(세로방향 5픽셀)와 정확히 일치한다.

☞ F(0, 82)에도 피크(peak)가 나타나는 것은 y축 방향으로 205/82 = 2.5 픽셀 간격의 주기 성분이 입력 이미지에 있다는 의미이다. 이는 이미지에 추가한 수평선이 정현파(sinusoidal wave)가 아니라 계단 형태이기 때문에 5 픽셀 주기의 정현파와 2.5 픽셀 주기의 정현파를 합쳐서 그러한 계단 형태를 근사했기 때문이다.

다음으로, 이번에는 그림 5(c)와 같이 대각선 방향의 정현파를 (a)의 이미지에 추가해 보자. 추가한 정현파는 x축 방향 주기 20 pixel, y축 방향 주기 10 픽셀인 2D sin 함수를 이용했다. 이 때, 푸리에 스펙트럼에는 F(10, 20.5)에 강한 피크(peak)가 생성됨을 확인할 수 있다. 즉, x축 방향으로는 W/u = 205/10 = 20.5 픽셀, y축 방향으로는 H/v = 205/20.5 = 10 픽셀의 주기 성분이 입력 이미지에 있음을 의미한다. 그리고 이는 실제 입력 이미지에 추가된 주기 성분과 정확히 일치한다 (소수점 오차는 u, v좌표를 정수로 표현함에 의한 것이다).

[역변환 과정]

이상으로 주파수 공간에서의 F(u,v)가 입력 이미지 공간에서 어떻게 연관되어 해석 될 수 있는지를 살펴보았다. 마지막으로 앞서 그림5 (b),(c)에서 스펙트럼의 피크 영역을 지운 후 푸리에 역변환을 하면 아래와 같은 재미있는 결과를 얻을 수 있다. [지운다는 의미는 해당되는 F(u,v) 값을 0으로 만든다는 의미]


<img width="604" alt="Screenshot 2022-05-09 at 20 56 43" src="https://user-images.githubusercontent.com/84698855/167487519-7ef9d0ae-738f-4410-983b-b22add76fdfe.png">


C. 푸리에 변환의 페이즈

푸리에 변환에서 스펙트럼은 잘 알려진 반면 페이즈는 상대적으로 잘 알려져 있지 않다. 하지만 페이즈에도 스펙트럼 못지 않은 중요한 정보가 담겨 있다. 
페이즈는 우리말로 번역하면 단계가 되고 전문용어로는 위상이 된다. 위키피디아 에는 페이즈를 반복되는 파형의 한 주기에서 첫 시작점의 각도 혹은 어느 한 순간의 위치라고 정의한다. 
즉, 파형의 시점이 어디인지가 페이즈이다. 예를들어 sin파와 cos파는 90도의 페이즈 차이가 존재하는 동일한 파형으로 볼 수있다. 

푸리에 변환의 관점에서 보면 페이즈는 원본 신호를 주기 신호로 분해했을 때 각 주기성분의 시점이 어디인지(즉, 각 주기성분들이 어떻게 줄을 맞춰서 원본 신호를 생성했는지)를 나타내는 요소가 된다. 



******************************************************************************************************************************************************

Convolution (합성곱)

*spatial filtering 
-> 필터링이란 영상에서 원하는 정보만 통과시키고 나머지는 걸러내는 작업이다. 이를 통해 잡음을 걸러내어 영상을 깔끔하게 만들거나 부드러운 느낌을 걸러내서 날카로운 느낌의 영상을 만들수 있다.

필터링은 마스크,커널,윈도우,필터 등으로 불리는 작은 크기의 행렬을 이용하는데 필터링 연산의 결과는 행렬의 모양과 원소의 값에 의해 결정되며 진한 색을 갖는 고정점을 기준 픽셀로 필터링 작업 수행 

필터 마스크 행렬은 모든 원소 합이 1 또는 0 이 되도록 설계하면 필터링 결과 영상의 평균 밝기가 입력 영상 평균 밝기와 같게 유지된다. 1보다 작을경우 영상의 평균 밝기가 어두워지고,
1보다크면 좀 더 밝아진다. 

* 합성곱을 사용하는 이유? 
-> 합성곱 식을 살펴보면 두 함수 f,g,가 있다. 일반적으로 사용될때 함수 f는 우리가 가지고 있는 본래의 신호,행렬,이미지 등이 되기도 한다.
이때 함수 g는 필터, 가중치 같은걸로 표현되기도 한다. 이 두 함수를 합성곱 하면? 주어진 신호, 행렬, 이미지를 우리가 원하는 함수로 만들어낼 수 있다. 

즉, 함수 f가 주어졌을때, 우리가 원하는 목적에 따라 함수 g를 선정하여 분해,변환,필터링 할 수 있다는 장점이 있다. 

한가지 예를들어보면, 딥러닝에서 유명한 CNN(convolution neural network) 가 있다. 조그만 필터로 이미지를 좌우상하 합성곱 하면서 그 부분의 특징값들을 뽑아낸다.
입력에 가까울 수록 가장자리, 곡선과 같은 저수준(low level)특징을 학습한다. 출력에 가까워 질 수록 질감,물체 일부분과 같은 고수준(high level)특징을 인식한다. 


* 합성곱과 푸리에 변환 관계 

만약 함수 f,g의 합성곱을 푸리에 변환하면 어떻게 될까? 
놀랍게도 복잡한 합성곱 연산이 두 함수를 각각 푸리에 변환한 것의 간단한 곱셈으로 표현된다. [공간영역 합성곱 = 주파수영역 곱셈] 


<img width="647" alt="Screenshot 2022-05-10 at 21 23 58" src="https://user-images.githubusercontent.com/84698855/167715597-8849adba-e7e4-4304-aab5-933678c525fa.png">


푸리에 변환 식에 합성곱식을 넣으면 위와 같은 식이 만들어진다. 연속함수 g를 보면 t변수가 있고 이는 푸리에 변환에서 동일한 t로 사용된다.[증명문제 스킵]




